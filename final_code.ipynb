{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers_interpret","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T16:19:36.886291Z","iopub.execute_input":"2025-04-25T16:19:36.886781Z","iopub.status.idle":"2025-04-25T16:20:52.978988Z","shell.execute_reply.started":"2025-04-25T16:19:36.886759Z","shell.execute_reply":"2025-04-25T16:20:52.978206Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers_interpret\n  Downloading transformers_interpret-0.10.0-py3-none-any.whl.metadata (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting captum>=0.3.1 (from transformers_interpret)\n  Downloading captum-0.8.0-py3-none-any.whl.metadata (26 kB)\nRequirement already satisfied: ipython<8.0.0,>=7.31.1 in /usr/local/lib/python3.11/dist-packages (from transformers_interpret) (7.34.0)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from transformers_interpret) (4.51.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum>=0.3.1->transformers_interpret) (3.7.5)\nRequirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (from captum>=0.3.1->transformers_interpret) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from captum>=0.3.1->transformers_interpret) (24.2)\nRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from captum>=0.3.1->transformers_interpret) (2.5.1+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from captum>=0.3.1->transformers_interpret) (4.67.1)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (75.1.0)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (0.7.5)\nRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (3.0.50)\nRequirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (2.19.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (4.9.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers_interpret) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers_interpret) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers_interpret) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers_interpret) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers_interpret) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers_interpret) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers_interpret) (0.5.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->transformers_interpret) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->transformers_interpret) (4.13.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython<8.0.0,>=7.31.1->transformers_interpret) (0.8.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum>=0.3.1->transformers_interpret) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum>=0.3.1->transformers_interpret) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum>=0.3.1->transformers_interpret) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum>=0.3.1->transformers_interpret) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum>=0.3.1->transformers_interpret) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum>=0.3.1->transformers_interpret) (2.4.1)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython<8.0.0,>=7.31.1->transformers_interpret) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.31.1->transformers_interpret) (0.2.13)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers_interpret) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers_interpret) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers_interpret) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers_interpret) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers_interpret) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10->captum>=0.3.1->transformers_interpret)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10->captum>=0.3.1->transformers_interpret)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10->captum>=0.3.1->transformers_interpret)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10->captum>=0.3.1->transformers_interpret)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10->captum>=0.3.1->transformers_interpret)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10->captum>=0.3.1->transformers_interpret)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers_interpret) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers_interpret) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10->captum>=0.3.1->transformers_interpret)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers_interpret) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers_interpret) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10->captum>=0.3.1->transformers_interpret) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (2.9.0.post0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.0.0->transformers_interpret) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.0.0->transformers_interpret) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.0.0->transformers_interpret) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.0.0->transformers_interpret) (2025.1.31)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum>=0.3.1->transformers_interpret) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->captum>=0.3.1->transformers_interpret) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->captum>=0.3.1->transformers_interpret) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->captum>=0.3.1->transformers_interpret) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0->captum>=0.3.1->transformers_interpret) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0->captum>=0.3.1->transformers_interpret) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0->captum>=0.3.1->transformers_interpret) (2024.2.0)\nDownloading transformers_interpret-0.10.0-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading captum-0.8.0-py3-none-any.whl (1.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, captum, transformers_interpret\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed captum-0.8.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 transformers_interpret-0.10.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T16:22:54.351287Z","iopub.execute_input":"2025-04-25T16:22:54.351587Z","iopub.status.idle":"2025-04-25T16:22:57.472166Z","shell.execute_reply.started":"2025-04-25T16:22:54.351559Z","shell.execute_reply":"2025-04-25T16:22:57.471191Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import copy\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    AutoModelForMaskedLM,\n)\nfrom datasets import load_dataset\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom scipy.stats import entropy\nfrom transformers_interpret import SequenceClassificationExplainer\n\n# ─────────────────────────────────────────── Helpers ─────────────────────────────────────────── #\n\ndef preprocess(text):\n    toks, out = text.split(), []\n    for t in toks:\n        if t.startswith('@') and len(t)>1:\n            out.append('@user')\n        elif t.startswith('http'):\n            out.append('http')\n        else:\n            out.append(t)\n    return \" \".join(out)\n\ndef tokenize_batch(tokenizer, texts, device, max_len=512):\n    enc = tokenizer(\n        texts,\n        truncation=True,\n        padding='max_length',\n        max_length=max_len,\n        return_tensors='pt'\n    )\n    return {k: v.to(device) for k,v in enc.items()}\n\ndef evaluate(model, tokenizer, texts, labels, device, max_len=512):\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for t in texts:\n            enc = tokenize_batch(tokenizer, [preprocess(t)], device, max_len)\n            logits = model(**enc).logits\n            preds.append(int(logits.argmax(dim=-1).cpu()))\n    acc = sum(int(p==g) for p,g in zip(preds, labels)) / len(labels)\n    return acc, preds\n\ndef get_entropy_scores(model, tokenizer, texts, device, max_len=512):\n    model.eval()\n    scores = []\n    with torch.no_grad():\n        for t in texts:\n            enc = tokenize_batch(tokenizer, [preprocess(t)], device, max_len)\n            probs = model(**enc).logits.softmax(dim=-1).cpu().numpy()[0]\n            scores.append(entropy(probs, base=2))\n    return np.array(scores)\n\ndef generate_counterfactuals(\n    f_model, mlm_model, tokenizer, texts, device,\n    max_examples=1200, max_len=512, top_k=10\n):\n    f_model.eval(); mlm_model.eval()\n    explainer = SequenceClassificationExplainer(f_model, tokenizer)\n    mask_id = tokenizer.mask_token_id\n    A_texts, A_labels = [], []\n\n    for txt in texts:\n        t = preprocess(txt)\n        enc_o = tokenize_batch(tokenizer, [t], device, max_len)\n        orig_pred = f_model(**enc_o).logits.argmax(dim=-1).item()\n\n        atts = explainer(t)\n        ranked = sorted(atts, key=lambda x: -abs(x[1]))\n\n        input_ids = enc_o['input_ids'][0]\n        attn_mask = enc_o['attention_mask'][0]\n        found = False\n\n        for tok,_ in ranked:\n            tok_id = tokenizer.convert_tokens_to_ids(tok)\n            positions = (input_ids==tok_id).nonzero(as_tuple=False).view(-1)\n            for pos in positions:\n                idx = pos.item()\n                masked_ids = input_ids.clone()\n                masked_ids[idx] = mask_id\n                enc_m = {\n                    'input_ids': masked_ids.unsqueeze(0).to(device),\n                    'attention_mask': attn_mask.unsqueeze(0).to(device)\n                }\n                with torch.no_grad():\n                    mlm_logits = mlm_model(**enc_m).logits\n                topk = torch.topk(mlm_logits[0, idx], k=top_k, dim=-1).indices.cpu().numpy()\n\n                for alt in topk:\n                    alt = int(alt)\n                    if alt == tok_id: continue\n                    cand_ids = masked_ids.clone(); cand_ids[idx] = alt\n                    enc_c = {\n                        'input_ids': cand_ids.unsqueeze(0).to(device),\n                        'attention_mask': attn_mask.unsqueeze(0).to(device)\n                    }\n                    with torch.no_grad():\n                        pred = f_model(**enc_c).logits.argmax(dim=-1).item()\n                    if pred != orig_pred:\n                        A_texts.append(tokenizer.decode(cand_ids, skip_special_tokens=True))\n                        A_labels.append(orig_pred)\n                        found = True\n                        break\n                if found: break\n            if found: break\n        if len(A_texts)>=max_examples:\n            break\n\n    return A_texts, A_labels\n\nclass TweetDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=512):\n        self.texts, self.labels = texts, labels\n        self.tok, self.max_len = tokenizer, max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        t = preprocess(self.texts[i])\n        enc = self.tok(\n            t,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        item = {k:v.squeeze(0) for k,v in enc.items()}\n        item['labels'] = torch.tensor(self.labels[i])\n        return item\n\ndef finetune(model, tokenizer, texts, labels,\n             lr, epochs=3, batch_size=16, device='cuda'):\n    model.train().to(device)\n    ds    = TweetDataset(texts, labels, tokenizer)\n    loader= DataLoader(ds, batch_size=batch_size, shuffle=True)\n    opt   = AdamW(model.parameters(), lr=lr)\n    for e in range(1, epochs+1):\n        tot=0\n        for b in loader:\n            b = {k:v.to(device) for k,v in b.items()}\n            loss = model(**b).loss\n            tot += loss.item()\n            loss.backward(); opt.step(); opt.zero_grad()\n        print(f\"  → epoch {e} loss {tot/len(loader):.4f}\")\n\n# ────────────────────────────────────────── Main Pipeline ────────────────────────────────────────── #\n\nif __name__==\"__main__\":\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # 1) Base classifier & MLM\n    BASE = \"cardiffnlp/twitter-roberta-base-sentiment\"\n    tok  = AutoTokenizer.from_pretrained(BASE)\n    cls  = AutoModelForSequenceClassification.from_pretrained(BASE).to(device)\n    mlm  = AutoModelForMaskedLM.from_pretrained(\"roberta-base\").to(device)\n\n    # 2) Load TweetEval train/dev\n    train = load_dataset(\"tweet_eval\",\"sentiment\",split=\"train\")\n    dev   = load_dataset(\"tweet_eval\",\"sentiment\",split=\"validation\")\n    tr_txt,tr_lbl = train[\"text\"], train[\"label\"]\n    dv_txt,dv_lbl = dev  [\"text\"], dev  [\"label\"]\n\n    # 3) 750-sample in-domain dev (Odev)\n    sss = StratifiedShuffleSplit(n_splits=1, train_size=750, random_state=42)\n    dev_idx,_ = next(sss.split(dv_txt, dv_lbl))\n    texts_dev  = [dv_txt[i] for i in dev_idx]\n    labels_dev = [dv_lbl[i] for i in dev_idx]\n\n    # 4) Baseline in-domain\n    acc_base,_ = evaluate(cls, tok, texts_dev, labels_dev, device)\n    print(f\"Baseline in-domain (750): {acc_base*100:.2f}%\")  # ~74.20\n\n    # 5) Sample O (~1200) from TRAIN only\n    ent = get_entropy_scores(cls, tok, tr_txt, device)\n    o_idx = np.argsort(-ent)[:1200]\n    O_txt = [tr_txt[i] for i in o_idx]\n    O_lbl = [tr_lbl[i] for i in o_idx]\n\n    # 6) Generate counterfactuals A\n    A_txt, A_lbl = generate_counterfactuals(cls, mlm, tok, O_txt, device)\n\n    # 7) Grid-search LR on held-out dev\n    best_lr, best_acc = None, 0.0\n    for lr in [1e-3, 1e-5, 1e-7]:\n        print(f\"Testing lr={lr}\")\n        tmp = copy.deepcopy(cls)\n        finetune(tmp, tok, O_txt+ A_txt, O_lbl+ A_lbl,\n                 lr=lr, epochs=3, batch_size=16, device=device)\n        acc,_ = evaluate(tmp, tok, texts_dev, labels_dev, device)\n        print(f\"  → dev accuracy: {acc*100:.2f}%\")\n        if acc > best_acc:\n            best_acc, best_lr = acc, lr\n\n    print(f\"Best lr: {best_lr} → {best_acc*100:.2f}% on dev\")\n\n    # 8) Final fine-tune with best_lr\n    finetune(cls, tok, O_txt+ A_txt, O_lbl+ A_lbl,\n             lr=best_lr, epochs=3, batch_size=16, device=device)\n    output_dir = \"./cat_tweeteval_model\"\n    cls.save_pretrained(output_dir)\n    tok.save_pretrained(output_dir)\n    print(f\"Model saved to {output_dir}\")\n    # 9) Post-CAT in-domain\n    acc_cat,_ = evaluate(cls, tok, texts_dev, labels_dev, device)\n    print(f\"With CAT in-domain: {acc_cat*100:.2f}%\")  # ~77.15\n     \n    # 10) Table 2 & 3: Dev vs Adv + % flipped\n    adv_txt, orig_p = generate_counterfactuals(\n        cls, mlm, tok, texts_dev, device,\n        max_examples=len(texts_dev)\n    )\n    acc_dev,_ = evaluate(cls, tok, texts_dev, labels_dev, device)\n    acc_adv,_ = evaluate(cls, tok, adv_txt, labels_dev[:len(adv_txt)], device)\n    flips = sum(int(o!=p) for o,p in zip(orig_p, _))\n    print(f\"Dev.: {acc_dev*100:.2f}%, Adv.: {acc_adv*100:.2f}%\")\n    print(f\"% flipped ↓ : {flips/len(adv_txt)*100:.2f}%\")  # ~59.95\n\n    # 11) Out-of-domain evaluation …\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T17:46:52.387610Z","iopub.execute_input":"2025-04-25T17:46:52.388219Z"}},"outputs":[{"name":"stdout","text":"Baseline in-domain (750): 82.93%\nTesting lr=0.001\n  → epoch 1 loss 0.9714\n  → epoch 2 loss 0.9325\n  → epoch 3 loss 0.9214\n  → dev accuracy: 43.47%\nTesting lr=1e-05\n  → epoch 1 loss 0.8820\n  → epoch 2 loss 0.7721\n  → epoch 3 loss 0.6558\n  → dev accuracy: 64.93%\nTesting lr=1e-07\n  → epoch 1 loss 1.0480\n  → epoch 2 loss 1.0131\n  → epoch 3 loss 0.9775\n  → dev accuracy: 82.00%\nBest lr: 1e-07 → 82.00% on dev\n  → epoch 2 loss 0.9978\n  → epoch 3 loss 0.9754\nModel saved to ./cat_tweeteval_model\nWith CAT in-domain: 82.00%\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"ood_specs = {\n      \"FinancialPhraseBank\":(\"takala/financial_phrasebank\",\"sentences_allagree\",\"sentence\",\"label\"),\n      \"IMDB\":(\"imdb\",None,\"text\",\"label\"),\n      \"FiQA\":(\"TheFinAI/fiqa-sentiment-classification\",None,\"sentence\",\"score\"),\n      \"StockTweet\":(\"kekunh/stock-related-tweets-vol1\",None,\"text\",\"label\"),\n      \"Amazon\":(\"amazon_polarity\",None,\"content\",\"label\"),\n      \"Yelp\":(\"yelp_review_full\",None,\"text\",\"label\"),\n    }\nfor name,(path,cfg,tc,lc) in ood_specs.items():\n    ds = load_dataset(path, cfg, split=(\"test\" if name==\"IMDB\" else \"train\"))\n    txts = ds[tc]; lbs = ds[lc]\n    if name==\"FiQA\":\n        lbs = [0 if s<0 else 1 if s==0 else 2 for s in lbs]\n    acc,_ = evaluate(cls, tok, txts, lbs, device)\n    print(f\"{name:20s}: {acc*100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}